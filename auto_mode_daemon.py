# -*- coding: utf-8 -*-
import os
import sys
import time
import json
import argparse
import subprocess
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any

# Root Path
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.append(str(PROJECT_ROOT))

# Import core modules
from src.ledger_manager import LedgerManager
from src.config import Config
from src.publisher import Publisher
from promotion_dispatcher import dispatch_publish, load_channel_config, repromote_best_sellers
from src.key_manager import KeyManager
from src.comment_bot import CommentBot
from src.error_learning_system import get_error_system
import requests
import re

# Logging configuration
LOGS_DIR = PROJECT_ROOT / "logs"
LOGS_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOGS_DIR / "auto_mode_daemon.log"

# Clear existing log file if it's very old or just for fresh start
if LOG_FILE.exists() and LOG_FILE.stat().st_size == 0:
    pass # Keep it

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8", delay=False),
        logging.StreamHandler(sys.stdout)
    ],
    force=True
)
logger = logging.getLogger("AutoModeDaemon")

STATUS_FILE = PROJECT_ROOT / "data" / "daemon_status.json"

def _utc_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def _update_status(update: Dict[str, Any]):
    status = {}
    if STATUS_FILE.exists():
        try:
            with open(STATUS_FILE, "r", encoding="utf-8") as f:
                status = json.load(f)
        except Exception:
            pass
    status.update(update)
    status["last_updated"] = _utc_iso()
    try:
        with open(STATUS_FILE, "w", encoding="utf-8") as f:
            json.dump(status, f, indent=2, ensure_ascii=False)
    except Exception as e:
        # íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨ëŠ” ë¡œê¹…í•˜ë˜ í¬ë˜ì‹œë˜ì§€ ì•Šë„ë¡ í•¨
        print(f"Status file write failed: {e}")
        pass 

def logger_info(msg: str):
    logger.info(msg)
    _update_status({"last_log": msg})

def _retry_pending_deployments():
    """ë°°í¬ ëŒ€ê¸° ì¤‘ì¸ ì œí’ˆë“¤(WAITING_FOR_DEPLOYMENT) ì¬ì‹œë„"""
    try:
        lm = LedgerManager(Config.DATABASE_URL)
        pub = Publisher(lm)
        waiting = lm.get_products_by_status("WAITING_FOR_DEPLOYMENT")
        
        if not waiting:
            return
            
        logger_info(f"ëŒ€ê¸° ì¤‘ì¸ ì œí’ˆ {len(waiting)}ê°œ ì¬ë°°í¬ ì‹œë„ ì¤‘...")
        
        # í”„ë¡œì íŠ¸ í•œë„ ë„ë‹¬ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ë¯¸ë¦¬ ì •ë¦¬ ì‹œë„
        try:
            pub.cleanup_old_projects(max_projects=150)
        except Exception as e:
            logger_info(f"Vercel í”„ë¡œì íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

        product_ids = []
        for p in waiting:
            pid = p["id"]
            output_dir = PROJECT_ROOT / "outputs" / pid
            if output_dir.exists():
                # ë°°í¬ ì „ ê²°ì œ ìœ„ì ¯ í™•ì¸ ë° ì£¼ì… (ê¸°ì¡´ ëˆ„ë½ë¶„ ëŒ€ì‘)
                index_html = output_dir / "index.html"
                if index_html.exists():
                    content = index_html.read_text(encoding="utf-8", errors="ignore")
                    if "startPay" not in content and "choose-plan" not in content and "crypto-payment-widget" not in content:
                        logger_info(f"[{pid}] ê²°ì œ ìœ„ì ¯ ëˆ„ë½ ê°ì§€. ì£¼ì… ì‹œë„...")
                        try:
                            from monetize_module import MonetizeModule, PaymentInjectConfig
                            mm = MonetizeModule()
                            mm.inject_payment_logic(str(index_html), PaymentInjectConfig(product_id=pid))
                            logger_info(f"[{pid}] ê²°ì œ ìœ„ì ¯ ì£¼ì… ì™„ë£Œ.")
                        except Exception as e:
                            logger_info(f"[{pid}] ê²°ì œ ìœ„ì ¯ ì£¼ì… ì‹¤íŒ¨: {e}")
                
                product_ids.append(pid)
        
        if product_ids:
            logger_info(f"ë°°ì¹˜ ë°°í¬ ì‹œì‘: {len(product_ids)}ê°œ ì œí’ˆ")
            # Batch publish call
            try:
                results = pub.publish_products_batch(product_ids)
                
                for pid, res in results.items():
                    if res.get("status") == "PUBLISHED":
                        logger_info(f"ì¬ë°°í¬ ì„±ê³µ: {pid}")
                    elif res.get("status") == "WAITING_VERIFICATION":
                        logger_info(f"ì¬ë°°í¬ ê²€ì¦ ëŒ€ê¸°: {pid} (URL: {res.get('url')})")
                    else:
                        err_msg = str(res.get("error", ""))
                        logger_info(f"ì¬ë°°í¬ ì‹¤íŒ¨: {pid} ({err_msg})")
            except Exception as e:
                logger_info(f"ë°°ì¹˜ ë°°í¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
    except Exception as e:
        logger_info(f"ì¬ë°°í¬ í”„ë¡œì„¸ìŠ¤ ì „ì²´ ì˜¤ë¥˜: {e}")

def _run_auto_heal():
    """ì‹¤íŒ¨í•œ ì œí’ˆë“¤ì„ ìë™ìœ¼ë¡œ ë³µêµ¬"""
    cmd = [sys.executable, "auto_heal_products.py"]
    logger_info("ìë™ ë³µêµ¬(Auto-heal) í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
    try:
        # ì‰˜ ì‹¤í–‰ ì‹œ ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ env ì„¤ì •
        env = os.environ.copy()
        env["PYTHONIOENCODING"] = "utf-8"
        subprocess.run(cmd, cwd=str(PROJECT_ROOT), shell=False, env=env)
        logger_info("ìë™ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ.")
    except Exception as e:
        logger_info(f"ìë™ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}")

def _promote_published_backlog():
    """PUBLISHED ìƒíƒœë¡œ ë‚¨ì•„ìˆëŠ” ì œí’ˆë“¤ í”„ë¡œëª¨ì…˜ ì‹¤í–‰"""
    try:
        from promotion_dispatcher import dispatch_publish
        lm = LedgerManager(Config.DATABASE_URL)
        products = lm.list_products()
        published = [p for p in products if p['status'] == 'PUBLISHED']
        
        if not published:
            return
            
        logger_info(f"í”„ë¡œëª¨ì…˜ ëŒ€ê¸° ì¤‘ì¸ ì œí’ˆ {len(published)}ê°œ ì²˜ë¦¬ ì‹œì‘...")
        _update_status({"phase": "promoting_backlog", "count": len(published)})
        for p in published:
            pid = p['id']
            _update_status({"phase": "promoting", "product_id": pid, "action": "Posting to channels"})
            try:
                res = dispatch_publish(pid)
                if res.get("dispatch_results", {}).get("wordpress", {}).get("ok"):
                    logger_info(f"í”„ë¡œëª¨ì…˜ ì„±ê³µ: {pid}")
                else:
                    logger_info(f"í”„ë¡œëª¨ì…˜ ì‹¤íŒ¨: {pid}")
                time.sleep(2)
            except Exception as e:
                logger_info(f"í”„ë¡œëª¨ì…˜ ì¤‘ ì˜ˆì™¸ ({pid}): {e}")
    except Exception as e:
        logger_info(f"í”„ë¡œëª¨ì…˜ ë°±ë¡œê·¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        _update_status({"phase": "idle", "product_id": None})

# -----------------------------
# Service Monitor
# -----------------------------
def _kill_port(port: int):
    """íŠ¹ì • í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ (Windows)"""
    if os.name != 'nt':
        return
    try:
        cmd = f"netstat -ano"
        output = subprocess.check_output(cmd, shell=True).decode('cp949', errors='ignore')
        for line in output.splitlines():
            if "LISTENING" in line and f":{port}" in line:
                parts = line.strip().split()
                if len(parts) >= 5:
                    pid = parts[-1]
                    if pid and pid != "0":
                        if int(pid) == os.getpid():
                            continue
                        subprocess.run(["taskkill", "/PID", pid, "/F", "/T"], capture_output=True)
    except Exception:
        pass

def _start_background_process(name: str, cmd: List[str]):
    """ë°±ê·¸ë¼ìš´ë“œë¡œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ë¡œê·¸ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜)"""
    log_path = LOGS_DIR / f"{name}.log"
    f = log_path.open("w", encoding="utf-8")
    try:
        env = os.environ.copy()
        env.pop("WERKZEUG_RUN_MAIN", None)
        env.pop("WERKZEUG_SERVER_FD", None)
        
        if os.name == 'nt':
            # Windows: CREATE_NO_WINDOW or similar if possible, but Popen defaults to hidden if shell=False and not GUI
            # actually standard Popen with stdout redirection is usually enough to not pop up if not using shell=True
            subprocess.Popen(
                cmd,
                cwd=str(PROJECT_ROOT),
                stdout=f,
                stderr=subprocess.STDOUT,
                stdin=subprocess.DEVNULL,
                shell=False,
                env=env,
                close_fds=True
            )
        else:
            subprocess.Popen(
                cmd,
                cwd=str(PROJECT_ROOT),
                stdout=f,
                stderr=subprocess.STDOUT,
                stdin=subprocess.DEVNULL,
                shell=False,
                env=env,
                start_new_session=True
            )
        logger_info(f"ğŸŸ¢ {name} started in background. Logs: {log_path}")
    except Exception as e:
        logger_info(f"Failed to start {name}: {e}")
        f.close()

def _check_and_start_services():
    """ê²°ì œ ì„œë²„(5000)ì™€ í”„ë¦¬ë·° ì„œë²„(8090)ê°€ ì£½ì–´ìˆìœ¼ë©´ ì‚´ë¦½ë‹ˆë‹¤."""
    services = [
            {"name": "Payment Server", "port": 5000, "script": "api/main.py", "url": "http://127.0.0.1:5000/health"},
            {"name": "Preview Server", "port": 8088, "script": "preview_server.py", "url": "http://127.0.0.1:8088/health"},
            {"name": "Dashboard", "port": 8099, "script": "dashboard_server.py", "url": "http://127.0.0.1:8099/health"},
        ]
    
    for svc in services:
        is_running = False
        try:
            r = requests.get(svc["url"], timeout=2)
            if r.status_code == 200:
                is_running = True
        except:
            pass
        
        if not is_running:
            logger_info(f"ğŸ”´ {svc['name']} is DOWN. Cleaning port {svc['port']} and restarting...")
            
            # 1. Kill zombie processes on the port
            _kill_port(svc['port'])
            time.sleep(1)
            
            # 2. Start process
            cmd = [sys.executable, svc["script"]]
            _start_background_process(svc["name"], cmd)
            
            time.sleep(5) # Wait for startup

    # Check Dashboard (Self-check not needed as we are daemon, but maybe check 8099?)
    try:
        r = requests.get("http://127.0.0.1:8099/health", timeout=2)
        if r.status_code == 200:
            # Trigger sync if dashboard is up
            try:
                requests.post("http://127.0.0.1:8099/api/system/sync_products", timeout=5)
            except:
                pass
    except:
        logger_info("ğŸŸ  Dashboard (8099) seems down. Attempting to restart...")
        _kill_port(8099)
        cmd = [sys.executable, "dashboard_server.py"]
        _start_background_process("dashboard", cmd)

def _run_autopilot(batch: int, topic: str, deploy: bool) -> Dict[str, Any]:
    cmd: List[str] = [sys.executable, "auto_pilot.py", "--batch", str(int(batch))]
    if topic:
        cmd += ["--topic", topic]
    
    _update_status({"phase": "running_autopilot", "last_cmd": cmd, "action": "Creating/Updating Product"})
    
    env = os.environ.copy()
    env["PYTHONIOENCODING"] = "utf-8"
    
    output_lines = []
    rc = 0
    
    try:
        # Run with Popen to capture and print output in real-time
        process = subprocess.Popen(
            cmd, cwd=str(PROJECT_ROOT), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env, bufsize=1, encoding='utf-8', errors='replace'
        )
        
        for line in process.stdout:
            print(line, end='')
            output_lines.append(line)
            
        process.wait()
        rc = process.returncode
        
        if rc != 0:
            logger_info(f"Auto-pilot failed with return code {rc}")
            try:
                error_system = get_error_system()
                context = f"Running auto_pilot.py with args: {cmd}"
                error_log = "".join(output_lines[-100:]) # Analyze last 100 lines
                analysis = error_system.analyze_and_fix(Exception(f"Process failed with RC {rc}. Log tail:\n{error_log}"), context=context)
                
                if analysis.get("confidence", 0) > 0.8:
                     logger_info(f"AI Suggested Fix for Auto-pilot: {analysis.get('details')}")
                     if error_system.apply_fix(analysis):
                         logger_info("Auto-fix applied. Retrying auto-pilot...")
                         # Retry once
                         process = subprocess.Popen(
                            cmd, cwd=str(PROJECT_ROOT), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env, bufsize=1, encoding='utf-8', errors='replace'
                         )
                         for line in process.stdout:
                             print(line, end='')
                         process.wait()
                         rc = process.returncode
            except Exception as e:
                logger_info(f"Error analysis failed: {e}")
                
    except Exception as e:
        logger_info(f"Subprocess execution failed: {e}")
        rc = -1
        
    _update_status({"phase": "autopilot_finished", "rc": rc})
    return {"rc": rc, "out": "Output logged"}

def _discover_new_products(since_ts: float) -> List[str]:
    outputs = PROJECT_ROOT / "outputs"
    if not outputs.exists():
        return []
    new_ids: List[str] = []
    for d in outputs.iterdir():
        if not d.is_dir():
            continue
        try:
            if d.stat().st_mtime >= since_ts:
                new_ids.append(d.name)
        except Exception:
            continue
    new_ids.sort()
    return new_ids

def _check_wordpress_comments():
    """ì›Œë“œí”„ë ˆìŠ¤ ëŒ“ê¸€ì„ í†µí•œ ìƒí’ˆ ì¬ìƒì„± ìš”ì²­ í™•ì¸"""
    try:
        cfg = load_channel_config()
        blog_cfg = cfg.get("blog", {})
        wp_api_url = blog_cfg.get("wp_api_url")
        wp_token = blog_cfg.get("wp_token")
        
        if not wp_api_url or not wp_token:
            return

        # ëŒ“ê¸€ ì—”ë“œí¬ì¸íŠ¸
        base_url = wp_api_url.split('/wp/v2/')[0] + '/wp/v2/comments'
        headers = {}
        if ":" in wp_token:
            import base64
            encoded_auth = base64.b64encode(wp_token.encode("utf-8")).decode("utf-8")
            headers["Authorization"] = f"Basic {encoded_auth}"
        else:
            headers["Authorization"] = f"Bearer {wp_token}"
            
        r = requests.get(base_url, headers=headers, params={"per_page": 10, "status": "approve"}, timeout=10)
        if r.status_code != 200:
            return
            
        comments = r.json()
        processed_file = PROJECT_ROOT / "data" / "processed_comments.json"
        processed_ids = []
        if processed_file.exists():
            try:
                processed_ids = json.loads(processed_file.read_text())
            except: pass

        lm = LedgerManager(Config.DATABASE_URL)
        new_processed = False
        
        for c in comments:
            c_id = c.get("id")
            if c_id in processed_ids:
                continue
            
            content = c.get("content", {}).get("rendered", "").lower()
            keywords = ["request", "recreate", "buy", "purchase", "ê²°ì œ", "êµ¬ë§¤", "ìš”ì²­", "ì‚´ê²Œìš”", "ì¬ìƒì„±"]
            
            if any(k in content for k in keywords):
                post_id = c.get("post")
                # í¬ìŠ¤íŠ¸ ë³¸ë¬¸ì—ì„œ product_id ì¶”ì¶œ ì‹œë„
                post_url = wp_api_url.split('/wp/v2/')[0] + f'/wp/v2/posts/{post_id}'
                pr = requests.get(post_url, headers=headers, timeout=10)
                if pr.status_code == 200:
                    p_data = pr.json()
                    p_content = p_data.get("content", {}).get("rendered", "")
                    p_slug = p_data.get("slug", "")
                    
                    product_id = None
                    # 1. data-product-id ì†ì„±
                    m = re.search(r'data-product-id=["\']([^"\']+)["\']', p_content)
                    if m: product_id = m.group(1)
                    # 2. ìŠ¬ëŸ¬ê·¸ì—ì„œ ID íŒ¨í„´ ì¶”ì¶œ
                    if not product_id:
                        m = re.search(r'(\d{8}-\d{6}-[a-zA-Z0-9\-]+)', p_slug)
                        if m: product_id = m.group(1)
                        
                        if product_id:
                            prod = lm.get_product(product_id)
                            # ìƒí’ˆì´ ì—†ê±°ë‚˜, ì‹¤íŒ¨ ìƒíƒœê±°ë‚˜, ëª…ì‹œì ìœ¼ë¡œ ì¬ìƒì„± ìš”ì²­ì´ ìˆëŠ” ê²½ìš° ì§„í–‰
                            should_recreate = False
                            if not prod:
                                should_recreate = True
                            elif prod.get("status") in ["PIPELINE_FAILED", "CRITICAL_FAILED", "QA_FAILED", "DELETED"]:
                                should_recreate = True
                            
                            if should_recreate:
                                logger_info(f"ëŒ“ê¸€ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ìƒí’ˆ ì¬ìƒì„± ì‹œì‘: {product_id}")
                                # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ë¥¼ ìœ„í•´ ì¦‰ì‹œ ì²˜ë¦¬ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ (ë˜ëŠ” ë³„ë„ ë½ íŒŒì¼ ì‚¬ìš© ê°€ëŠ¥)
                                # ì¬ìƒì„± íŠ¸ë¦¬ê±° (auto_pilot í˜¸ì¶œ, product_id ì „ë‹¬)
                                parts = product_id.split('-', 2)
                                topic = parts[2] if len(parts) > 2 else "requested"
                                subprocess.Popen([sys.executable, "auto_pilot.py", "--batch", "1", "--topic", topic, "--product_id", product_id, "--deploy", "1"])
                            else:
                                logger_info(f"ëŒ“ê¸€ ìš”ì²­ì´ ìˆìœ¼ë‚˜ ìƒí’ˆì´ ì´ë¯¸ ì–‘í˜¸í•œ ìƒíƒœì…ë‹ˆë‹¤: {product_id} ({prod.get('status')})")
            
            processed_ids.append(c_id)
            new_processed = True
            
        if new_processed:
            processed_file.write_text(json.dumps(processed_ids))
            
    except Exception as e:
        logger_info(f"WordPress ëŒ“ê¸€ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        try:
            error_system = get_error_system()
            analysis = error_system.analyze_and_fix(e, context="Checking WordPress comments")
            if analysis.get("confidence", 0) > 0.8 and error_system.apply_fix(analysis):
                logger_info("Auto-fix applied. Retrying comment check...")
                _check_wordpress_comments()
        except Exception as ai_e:
            logger_info(f"AI error analysis failed: {ai_e}")

def _run_system_audit():
    """ì‹œìŠ¤í…œ ìƒì‹œ ê²€ìˆ˜ ì‹¤í–‰"""
    logger_info("ì‹œìŠ¤í…œ ìƒì‹œ ê²€ìˆ˜ ë´‡ ê°€ë™...")
    try:
        from src.audit_bot import SystemAuditBot
        bot = SystemAuditBot()
        report = bot.run_full_audit()
        healthy = report["summary"]["healthy_products"]
        total = report["summary"]["total_products"]
        logger_info(f"ê²€ìˆ˜ ì™„ë£Œ: ì •ìƒ ìƒí’ˆ {healthy}/{total}")
    except Exception as e:
        logger_info(f"ì‹œìŠ¤í…œ ê²€ìˆ˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        try:
            error_system = get_error_system()
            analysis = error_system.analyze_and_fix(e, context="Running system audit")
            if analysis.get("confidence", 0) > 0.8 and error_system.apply_fix(analysis):
                logger_info("Auto-fix applied. Retrying system audit...")
                _run_system_audit()
        except Exception as ai_e:
            logger_info(f"AI error analysis failed: {ai_e}")

def _run_market_analysis():
    """ì‹œì¥ ë¶„ì„ ë° ê°€ê²© ìµœì í™” ì‹¤í–‰"""
    logger_info("ì‹œì¥ ë¶„ì„ ë° ê°€ê²© ìµœì í™” ë´‡ ê°€ë™...")
    try:
        from src.market_analyzer import MarketAnalyzer
        from src.ledger_manager import LedgerManager
        from src.config import Config
        
        analyzer = MarketAnalyzer(PROJECT_ROOT)
        stats, updated_ids = analyzer.analyze_and_optimize()
        
        if stats:
            logger_info(f"ê°€ê²© ìµœì í™” ì™„ë£Œ: {json.dumps(stats, ensure_ascii=False)}")
            
        # ì—…ë°ì´íŠ¸ëœ ì œí’ˆì´ ìˆìœ¼ë©´ ìƒíƒœë¥¼ WAITING_FOR_DEPLOYMENTë¡œ ë³€ê²½í•˜ì—¬ ì¬ë°°í¬ ìœ ë„
        if updated_ids:
            logger_info(f"ì—…ë°ì´íŠ¸ëœ ì œí’ˆ {len(updated_ids)}ê°œë¥¼ ì¬ë°°í¬ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
            try:
                lm = LedgerManager(Config.DATABASE_URL)
                for pid in updated_ids:
                    # í˜„ì¬ ìƒíƒœê°€ PUBLISHEDì¸ ê²½ìš°ì—ë§Œ ì¬ë°°í¬ ëŒ€ê¸°ë¡œ ë³€ê²½ (ì‹¤íŒ¨í•œ ê²ƒì€ ë†”ë‘ )
                    prod = lm.get_product(pid)
                    if prod and prod.get("status") == "PUBLISHED":
                        lm.update_product(pid, status="WAITING_FOR_DEPLOYMENT")
                        logger_info(f"[{pid}] ê°€ê²© ë³€ë™ìœ¼ë¡œ ì¸í•œ ì¬ë°°í¬ ìš”ì²­ë¨.")
            except Exception as e:
                logger_info(f"ì¬ë°°í¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                
    except Exception as e:
        logger_info(f"ì‹œì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        try:
            error_system = get_error_system()
            analysis = error_system.analyze_and_fix(e, context="Running market analysis")
            if analysis.get("confidence", 0) > 0.8 and error_system.apply_fix(analysis):
                logger_info("Auto-fix applied. Retrying market analysis...")
                _run_market_analysis()
        except Exception as ai_e:
            logger_info(f"AI error analysis failed: {ai_e}")

def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--interval", type=int, default=3600, help="seconds between runs")
    ap.add_argument("--batch", type=int, default=1, help="products per run")
    ap.add_argument("--topic", type=str, default="", help="optional topic, blank=auto")
    ap.add_argument("--deploy", type=int, default=0, help="1 to deploy to vercel")
    ap.add_argument(
        "--publish",
        type=int,
        default=1,
        help="1 to create ready_to_publish and optionally webhook post",
    )
    ap.add_argument("--max_runs", type=int, default=0, help="0=forever")
    args = ap.parse_args()

    interval = max(60, int(args.interval))
    batch = max(1, int(args.batch))
    topic = str(args.topic or "").strip()
    deploy = bool(int(args.deploy))
    publish = bool(int(args.publish))
    max_runs = int(args.max_runs)

    # 0. ì´ˆê¸° í‚¤ ìŠ¤ìº” (Auto Key Extraction)
    try:
        km = KeyManager(PROJECT_ROOT)
        km.scan_and_extract()
    except Exception as e:
        logger_info(f"í‚¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    # 0.5 ëŒ“ê¸€ ë´‡ ì´ˆê¸°í™”
    comment_bot = None
    try:
        # Load secrets fresh
        with open(PROJECT_ROOT / "data" / "secrets.json", "r", encoding="utf-8") as f:
            secrets = json.load(f)
        if secrets.get("WP_API_URL") and secrets.get("WP_TOKEN"):
            comment_bot = CommentBot(secrets["WP_API_URL"], secrets["WP_TOKEN"])
            logger_info("ğŸ¤– ëŒ“ê¸€ ê´€ë¦¬ ë´‡(CommentBot) í™œì„±í™”ë¨")
    except Exception as e:
        logger_info(f"ëŒ“ê¸€ ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨ (ìŠ¤í‚µ): {e}")

    run_count = 0
    consecutive_failures = 0
    MAX_CONSECUTIVE_FAILURES = 5

    logger_info(f"DAEMON STARTED: interval={interval}s, batch={batch}, topic='{topic}'")
    _update_status({"status": "running", "pid": os.getpid(), "start_time": _utc_iso()})

    try:
        while True:
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                logger_info(f"ì—°ì† {consecutive_failures}íšŒ ì‹¤íŒ¨ ë°œìƒ. ì•ˆì „ì„ ìœ„í•´ ë°ëª¬ì„ 1ì‹œê°„ ë™ì•ˆ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                _update_status({"status": "paused", "reason": "too_many_failures"})
                time.sleep(3600)
                consecutive_failures = 0
                continue

            run_count += 1
            if max_runs > 0 and run_count > max_runs:
                logger_info(f"Max runs ({max_runs}) reached. Exiting.")
                break

            start_time = time.time()
            logger_info(f"--- RUN #{run_count} START ---")
            
            try:
                # -1. ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬ ë° ìë™ ì‹œì‘
                _check_and_start_services()

                # 0. ì›Œë“œí”„ë ˆìŠ¤ ëŒ“ê¸€ í™•ì¸ (ì¬ìƒì„± ìš”ì²­)
                _check_wordpress_comments()

                # 1. ì¬ë°°í¬ ì‹œë„ (Vercel í•œë„ ë“±ìœ¼ë¡œ ë°€ë¦° ê²ƒë“¤)
                _retry_pending_deployments()

                # 2. ì‹œìŠ¤í…œ ìƒíƒœ ê²€ìˆ˜ (ë°°í¬ í›„ ìƒíƒœ í™•ì¸ ë° ìë™ ë³µêµ¬ íŠ¸ë¦¬ê±°)
                # _run_system_audit() ì€ run_countê°€ 5ì˜ ë°°ìˆ˜ì¼ ë•Œë§Œ ì‹¤í–‰ (ë§¤ ì‹œê°„ 1íšŒ ì •ë„)
                if run_count % 5 == 1:
                    _run_system_audit()
                
                # 2.5 ì‹œì¥ ë¶„ì„ ë° ê°€ê²© ìµœì í™” (ë§¤ íšŒ ì‹¤í–‰í•˜ì—¬ ê°€ê²© ë™ê¸°í™” ìœ ì§€)
                _run_market_analysis()

                # 3. í—¬ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±
                try:
                    subprocess.run([sys.executable, "generate_health_report.py"], check=False)
                except:
                    pass
                
                # 3.5 í”„ë¡œëª¨ì…˜ ë°±ë¡œê·¸ ì²˜ë¦¬ (ëˆ„ë½ëœ í™ë³´ ìë™ ìˆ˜í–‰)
                _promote_published_backlog()

                # 4. ì˜¤í† íŒŒì¼ëŸ¿ ì‹¤í–‰ (ìƒì„± -> ë°°í¬)
                res = _run_autopilot(batch, topic, deploy)
                logger_info(f"Autopilot finished (rc={res['rc']})")
                
                if res['rc'] == 0:
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    logger_info(f"Autopilot failed. ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜: {consecutive_failures}")

                # 5. ìƒˆë¡œìš´ ì œí’ˆë“¤ í™ë³´ ì±„ë„ë¡œ ë°œí–‰ (WordPress, X ë“±)
                _promote_published_backlog()

                # 5.5 ì„±ê³¼ ê¸°ë°˜ ì¬í™ë³´ (3íšŒ ì‹¤í–‰ë§ˆë‹¤ 1ë²ˆ)
                if run_count % 3 == 0:
                    logger_info("ì„±ê³¼ ê¸°ë°˜ ì¬í™ë³´(Analytics Loop) ì‹¤í–‰...")
                    try:
                        repromote_best_sellers()
                    except Exception as e:
                        logger_info(f"ì¬í™ë³´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

                # 5.6 ëŒ“ê¸€ ìë™ ì‘ë‹µ (ë§¤ ì‹¤í–‰ë§ˆë‹¤)
                if comment_bot:
                    logger_info("ëŒ“ê¸€ ë´‡ ì‹¤í–‰ ì¤‘...")
                    try:
                        comment_bot.run_cycle()
                    except Exception as e:
                        logger_info(f"ëŒ“ê¸€ ë´‡ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

                if publish:
                    new_pids = _discover_new_products(start_time)
                    if new_pids:
                        logger_info(f"ë°œê²¬ëœ ìƒˆ ì œí’ˆ {len(new_pids)}ê°œ í™ë³´ ë°œí–‰ ìƒíƒœ í™•ì¸/ì—…ë°ì´íŠ¸...")
                        for pid in new_pids:
                            try:
                                # promotion_dispatcherë¥¼ í†µí•´ WordPress, X ë“±ìœ¼ë¡œ ì „ì†¡ (ì´ë¯¸ ë˜ì–´ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸)
                                dispatch_publish(pid)
                                logger_info(f"í™ë³´ ë°œí–‰ ì™„ë£Œ: {pid}")
                            except Exception as e:
                                logger_info(f"í™ë³´ ë°œí–‰ ì¤‘ ì˜¤ë¥˜ ({pid}): {e}")

                # 6. ìë™ ë³µêµ¬ ì‹œë„
                _run_auto_heal()

                # 7. ì£¼ê¸°ì ì¸ ë°°í¬ í”„ë¡œì íŠ¸ ì •ë¦¬ (Git Push ëª¨ë“œì—ì„œëŠ” ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ë¹„í™œì„±í™”)
                # try:
                #     from src.publisher import Publisher
                #     from src.ledger_manager import LedgerManager
                #     from src.config import Config
                #     pub = Publisher(LedgerManager(Config.DATABASE_URL))
                #     pub.cleanup_old_projects(max_projects=190) # í•œë„ë¥¼ 190ìœ¼ë¡œ ì™„í™” (ì‹¤ì œ í•œë„ 200)
                # except Exception as e:
                #     logger_info(f"Vercel ì •ê¸° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            except Exception as run_err:
                consecutive_failures += 1
                logger_info(f"ë£¨í‹´ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {run_err}. ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜: {consecutive_failures}")

            elapsed = time.time() - start_time
            wait_sec = max(10, interval - elapsed)
            logger_info(f"--- RUN #{run_count} END (took {elapsed:.1f}s). Next run in {wait_sec:.1f}s ---")
            
            _update_status({
                "phase": "sleeping",
                "last_run_end": _utc_iso(),
                "next_run_approx": datetime.fromtimestamp(time.time() + wait_sec, timezone.utc).isoformat()
            })
            
            # Smart sleep with service monitoring
            next_run_time = time.time() + wait_sec
            while time.time() < next_run_time:
                remaining = next_run_time - time.time()
                # Check services every 60 seconds or remaining time
                sleep_chunk = min(remaining, 60)
                if sleep_chunk <= 0:
                    break
                time.sleep(sleep_chunk)
                
                # Periodic service check
                try:
                    _check_and_start_services()
                except Exception:
                    pass

    except KeyboardInterrupt:
        logger_info("Daemon stopped by user.")
    except Exception as e:
        logger_info(f"Daemon crashed: {e}")
        import traceback
        logger_info(traceback.format_exc())
        return 1
    finally:
        _update_status({"status": "stopped", "stop_time": _utc_iso()})

    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception as e:
        print(f"FATAL ERROR in Auto Mode Daemon: {e}")
        import traceback
        traceback.print_exc()
        input("Press Enter to exit...")
        sys.exit(1)
